REFERENCES.

[1]. Jerry Dean, Sanjay Ghemawat, “MapReduce: Simplified Data Processing on Large Clusters”, OSDI 04: 6th Symposium on Operating Systems Design and Implementation.
[2]. Matei Zaharia, Andrew Konwinski, "Improving MapReduce Performance in heterogenous Environments", Electrical Engineering and Computer Sciences, University of California at Berkeley, Technical Report No. UCB/EECS-2008-99, page 14, 2008.
[3]. Dawei Jiang, Beng Chin, Ooi Lei, Shi Sai Wu, “The Performance of MapReduce: An In-depth Study”, School of Computing, National
University of Singapore, page 2, 2010.
[4]. Tom White, "Hadoop: The Definitive Guide, 2nd Edition": pages 176-177, 2010.

PROBLEM STATEMENT.

Over the years, the advent of internet, advances in computing power, storage and networking technologies, have led to the creation of vast amounts of information, and have improved the way we process it.  This information may be generated from digital media, web authoring, scientific instruments, physical simulations, data mining etc.  It becomes a challenge to efficiently store, query, analyze, understand and utilize these immense datasets [1].
Hadoop MapReduce is a software framework that can be used to easily develop applications that process very large ammounts of data-sets in parallel using clusters of commodity hardware in a reliable manner. It provides abstract design patterns for simplifying development of data-center scale computer systems with high storage and processing demands in a cost effective way. The advantages it offers include fine grained fault tolerence, an architecture that provides a simple yet expressive programming model that can be used to express a wide range of analytical tasks and storage independent design that enables elastic scalability, increased throughput by using data locality whereby the data is processed on the same machine it is stored. 

A developer basically supplys a selected algorithm and selected dataset to the mapReduce interface provided by Hadoop API as a job, the runtime hadoop system then splits the dataset into evenly sized chunks, transforms them into key/value pairs, and then distributes them amongest a series of parallel map tasks for processing, every map task that completes produces intermediate key/value pairs as output, each of which is assigned a reduce task that applys a reduce function and then finally producing the final out put key/value pairs. In between each map phase and reduce phase is a shuffle step that sorts all map output values with the same key into a single reduce input pair.

Although hadoop has gained popularity as one of the most efficient data-parallel computing platforms currently available, the core framework has got features and areas that need further research in order to improve its overall performance, for instance [3] notes that, the default sort-merge grouping algorithm used within MapReduce is not efficient for certain kinds of analytical tasks such as aggregation and equal-join, they observe that different grouping algorithms can sigificantly improve performance, but the Hadoop core may need to be modified for an efficient grouping algorithm to be implemented. 

[2] Mentions that, the mapReduce scheduling algorithms Fair Scheduler and capacity Scheduler [4], which are responsible for speculative execution of tasks, could be more intelligent when deciding how many tasks to simultaneously assign to each slave, and also which nodes to use for assigning speculative tasks. They [2] further believe that there would be benefits if a simpler scheduler algorithm that attempts to adapt to the differing capabilities of each slave by using an adaptive task assignment algorithm similar to TCP’s slow-start, congestion control and backoff policies, were to be developed.
 

---------------------------------------------------------------------------------------------------------------------




---------------------------------------------------------------------------------------------------------------------
What are the advantages of hadoop?

it abstracts coordination of the proceses running in the distributed computation away from the developer.

fine grained fault tolerance - it hides the complexity of fault tolerance away from a developer, in that if one of the nodes crashes, mapReduce automatically assigns its tasks to other nodes in the cluster.

elastic scalability - it efficeintly uses all the compute resources it has for job execution. 
simple yet expressive prorgamming model - mapReduce jobs can be used to express a wide range of analytical tasks, like datamining, machine learning, graph processing etc.

Storage independent design - The model can process data of any kind, structured or unstructured,

data locality - the data is processed and stored on the same machine, thus increasing throughput and reducing costs on expensive networking resourses.

speculative execution - basically when the runtime system senses that any of the nodes is slow or performing poorly, mapReduce runs a speculative copy of its tasks on a separate node in order to speed up the computation.


The Hadoop Scheduler (hadoop Scheduling Algorithm)is responsible for handling speculative execution.


I/O mode, indexing, Grouping schemes, block-level scheduling, 

Data parsing - This is when a reader retrieves raw data from the storage system and converts it to key/value pairs. 

	MapReduce requires the underlying storage system to provide I/O interfaces for scanning data. There are two types of modes, Direct I/O and Streaming I/O. Direct I/O out performs streaming I/O. [p2]
	The Record Decording schemes used by mapReduce include mutable and immutable decoding, mutable decoding scheme is much faster when using it for the selection task for database like work.
	By exploiting the index, mapReduce can gain improved performance while processing data (Record Parsing).
	The mapReduce programming model provides a default sort-merge grouping algorithm (Data Transformation Logic) that is not effecient for certain analytical tasks like aggregation and joins, thus calls for new or different grouping algorithms to be implemented in hadoop core. 
	The Block level scheduling algorithm used by MapReduce in the assignment of map tasks to available nodes affects performance, since it is sensitive to the processing speed of slave nodes. also the scheduler incurs higher costs while scheduling more data blocks.

---------------------------------------------------------------------------------------------------------------------

RESEARCH QUESTIONS.

what are the features of mapReduce?


---------------------------------------------------------------------------------------------------------------------
how is mapReduce used?

it is used a general data-processing tool
Several algorithms can be expressed in MapReduce, these include image analysis, machine learning, 
internet-scale search engines

---------------------------------------------------------------------------------------------------------------------
What is the execution flow of a MapReduce Job?
	actors in MapReduce System - Job, JobClient, JobTracker, TaskTracker, MapTask, ReduceTask, Scheduler


---------------------------------------------------------------------------------------------------------------------
what factors affect the performance of mapReduce?


---------------------------------------------------------------------------------------------------------------------
how does it achieve greater performance?


---------------------------------------------------------------------------------------------------------------------
which components are used within it?


---------------------------------------------------------------------------------------------------------------------
how does mapReduce work?


---------------------------------------------------------------------------------------------------------------------
how are different people using it?


---------------------------------------------------------------------------------------------------------------------
how is map input data partitioned?

org.apache.hadoop.mapreduce.InputFormat subclass 


---------------------------------------------------------------------------------------------------------------------
how map tasks manage, sort and suffle results?


---------------------------------------------------------------------------------------------------------------------
how is scheduling done?


---------------------------------------------------------------------------------------------------------------------
how is distribution done?


---------------------------------------------------------------------------------------------------------------------
how is parallelisation done?


---------------------------------------------------------------------------------------------------------------------
how is the intermediate data produced by the map() function grouped for the reduce() funtion to process? 


---------------------------------------------------------------------------------------------------------------------
which similar alternatives exist in the distributed computing landscape? comparisons with other systems?

MapReduce is a batch-query processor [4 pg4] mainly used for large-scale batch processing and analysis.  

RDBMSs
Grid Computing

Higer Performance Computing, HPC, distributed computing on clusters of machines connected over a network, accessing shared data stored on a SAN, its good for compute intensive jobs, but lags behind when very large amounts of data need to be processsed, MapReduce happens to get over this barrier by using data locality, in which it collocates the data with the compute nodes. [4 pg7]

Message Passing Interface, MPI programs, they manage their own checkpointing and failure recovery, giving greater control to the developer over data flow and coordination of the computation, by using low level C constructs and high level algorithms for analysis, only drawback to this is complexity involved, which map reduce abstracts from the developer. [4 pg7]

 
Google MapReduce
Hadoop MapReduce
Dryad

---------------------------------------------------------------------------------------------------------------------
